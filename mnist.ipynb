{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_idx_images(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert magic == 2051, f\"Magic number mismatch, got {magic}\"\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # flat the data to N * 784 matrix and change to float32\n",
    "        images = (data.reshape(num, rows*cols)).astype(np.float32) / 255.0\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_idx_labels(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "        assert magic == 2049, f\"Magic number mismatch, got {magic}\"\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels: np.ndarray):\n",
    "    targets = np.zeros((labels.size, 10), dtype=np.float32)\n",
    "    #生成一个长度为labels长度的索引数列\n",
    "    index_matrix= np.arange(labels.size)\n",
    "    #用labels向量的个数来做行索引，用它的值来做列索引\n",
    "    targets[index_matrix, labels] = 1\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播函数，得到Z\n",
    "def forward(X: np.ndarray, W: np.ndarray, b: np.ndarray):\n",
    "    Z = X.dot(W) + b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax处理，把Z变成概率矩阵\n",
    "def softMax(Z: np.ndarray):\n",
    "    Z_shift = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp_Z = np.exp(Z_shift)\n",
    "    probs = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算损失函数的Y_onehot版本\n",
    "def cross_entropy_from_onehot(Y_hat: np.ndarray, Y_onehot: np.ndarray, eps= 1e-12):\n",
    "    logp = np.log(Y_hat + eps) #给矩阵每个元素加一个微小的eps，避免无穷，然后把每个数变成log\n",
    "    # 逐元素相乘：把 logp 和 Y_onehot 对位相乘。\n",
    "    loss = -np.sum(Y_onehot * logp) / Y_hat.shape[0] #把矩阵所有值相加，但是不为零的只有对应位置，所以是所有正确概率log相加除以个数\n",
    "    # 得到的就是损失函数\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算损失函数的y_int版本\n",
    "def cross_entropy_from_int(Y_hat: np.ndarray, y_int: np.ndarray, eps: int=1e-12):\n",
    "    # 生成一个y_hat长度的index索引\n",
    "    rows = np.arange(Y_hat.shape[0])\n",
    "    # 使用高级索引，得到y_hat中所有需要得到的值，也就是对应正确答案的概率\n",
    "    p_true = Y_hat[rows, y_int]\n",
    "    # 把所有正确答案加eps求log，然后求负平均值\n",
    "    loss = -np.mean(np.log(p_true + eps))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算损失函数对Zt的导数，作为求得梯度的前提\n",
    "def d_loss_d_Z(Y_hat: np.ndarray, Y_onehot: np.ndarray, B: int):\n",
    "    G = Y_hat.copy()\n",
    "    G -= Y_onehot\n",
    "    G /= B\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件\n",
    "images = load_idx_images(\"./train-images.idx3-ubyte\")\n",
    "labels = load_idx_labels(\"./train-labels.idx1-ubyte\")\n",
    "\n",
    "# 设定训练样本大小\n",
    "batch_size = 128\n",
    "# 设定训练轮次\n",
    "epochs = 20\n",
    "# 设定学习率\n",
    "N = images.shape[0]\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建参数矩阵\n",
    "rows, cols = 784, 10\n",
    "W = (np.random.randn(rows, cols) * 0.01).astype(np.float32)\n",
    "b = np.zeros((1, 10), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #求的概率矩阵\n",
    "# Y_hat= softMax(Z)\n",
    "# assert np.allclose(Y_hat.sum(axis=1), 1.0, atol=1e-6)\n",
    "# G = d_loss_d_Z(Y_hat, Y_onehot, B)\n",
    "# # 最重要的，得到对于W和对于b的偏导数，从而得到梯度\n",
    "# grad_W = Xb.T.dot(G)\n",
    "# grad_b = G.sum(axis=0, keepdims=True)\n",
    "# W -= lr * grad_W\n",
    "# b -= lr * grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1: loss=0.4021, acc=0.8917\n",
      "round 2: loss=0.3517, acc=0.9032\n",
      "round 3: loss=0.3305, acc=0.9073\n",
      "round 4: loss=0.3188, acc=0.9105\n",
      "round 5: loss=0.3087, acc=0.9141\n",
      "round 6: loss=0.3019, acc=0.9164\n",
      "round 7: loss=0.2979, acc=0.9168\n",
      "round 8: loss=0.2929, acc=0.9188\n",
      "round 9: loss=0.2905, acc=0.9192\n",
      "round 10: loss=0.2869, acc=0.9209\n",
      "round 11: loss=0.2847, acc=0.9210\n",
      "round 12: loss=0.2823, acc=0.9217\n",
      "round 13: loss=0.2796, acc=0.9226\n",
      "round 14: loss=0.2792, acc=0.9229\n",
      "round 15: loss=0.2765, acc=0.9239\n",
      "round 16: loss=0.2757, acc=0.9242\n",
      "round 17: loss=0.2734, acc=0.9245\n",
      "round 18: loss=0.2718, acc=0.9247\n",
      "round 19: loss=0.2708, acc=0.9251\n",
      "round 20: loss=0.2704, acc=0.9249\n",
      "round 21: loss=0.2689, acc=0.9255\n",
      "round 22: loss=0.2679, acc=0.9255\n",
      "round 23: loss=0.2671, acc=0.9262\n",
      "round 24: loss=0.2662, acc=0.9262\n",
      "round 25: loss=0.2657, acc=0.9264\n",
      "round 26: loss=0.2652, acc=0.9261\n",
      "round 27: loss=0.2642, acc=0.9272\n",
      "round 28: loss=0.2637, acc=0.9274\n",
      "round 29: loss=0.2629, acc=0.9277\n",
      "round 30: loss=0.2618, acc=0.9278\n",
      "round 31: loss=0.2615, acc=0.9278\n",
      "round 32: loss=0.2612, acc=0.9276\n",
      "round 33: loss=0.2615, acc=0.9284\n",
      "round 34: loss=0.2601, acc=0.9281\n",
      "round 35: loss=0.2606, acc=0.9277\n",
      "round 36: loss=0.2590, acc=0.9286\n",
      "round 37: loss=0.2592, acc=0.9282\n",
      "round 38: loss=0.2587, acc=0.9286\n",
      "round 39: loss=0.2576, acc=0.9287\n",
      "round 40: loss=0.2564, acc=0.9297\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    indices = np.random.permutation(N)\n",
    "    X_shuffled = images[indices]\n",
    "    Y_shuffled = labels[indices]\n",
    "    \n",
    "    for start in range(0, N, batch_size):\n",
    "        end = start + batch_size\n",
    "        Xb = X_shuffled[start:end]\n",
    "        Yb = Y_shuffled[start:end]\n",
    "        B = Xb.shape[0] \n",
    "\n",
    "        # forward \n",
    "        Z = Xb @ W + b\n",
    "        probs = softMax(Z)\n",
    "\n",
    "        # reverse\n",
    "        Y_onehot = one_hot(Yb)\n",
    "        G = (probs - Y_onehot) / B\n",
    "        grad_W = Xb.T @ G\n",
    "        grad_b = G.sum(axis=0, keepdims=True)\n",
    "\n",
    "        # update W and b\n",
    "        W -= lr * grad_W\n",
    "        b -= lr * grad_b\n",
    "\n",
    "    Z_all = images @ W + b\n",
    "    probs_all = softMax(Z_all)\n",
    "    loss = cross_entropy_from_int(probs_all, labels)\n",
    "    acc = (probs_all.argmax(axis = 1) == labels).mean()\n",
    "    print(f\"round {epoch + 1}: loss={loss:.4f}, acc={acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test result: loss: 0.267029, acc: 0.924100.\n"
     ]
    }
   ],
   "source": [
    "X_test_path = \"./t10k-images.idx3-ubyte\"\n",
    "Y_test_path = \"./t10k-labels.idx1-ubyte\"\n",
    "X_test = load_idx_images(X_test_path)\n",
    "Y_test = load_idx_labels(Y_test_path)\n",
    "\n",
    "Z_test = X_test @ W + b\n",
    "probs_test = softMax(Z_test)\n",
    "loss_test = cross_entropy_from_int(probs_test, Y_test)\n",
    "acc_test = (probs_test.argmax(axis=1) == Y_test).mean()\n",
    "print(f\"\\ntest result: loss: {loss_test:4f}, acc: {acc_test:4f}.\")\n",
    "np.save(\"W.npy\", W)\n",
    "np.save(\"b.npy\", b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
